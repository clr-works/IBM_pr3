{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **torch.export() and AOTInductor: Overview and Demo**\n",
        "\n",
        "This notebook provides an overview of `torch.export()` in PyTorch, covering its usage, limitations, and how it compares to alternatives like `torch.compile()`. We’ll also explore methods for handling non-traceable components and dynamic input shapes.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### 1. Overview of `torch.export()`\n",
        "\n",
        "`torch.export()` is an experimental feature in PyTorch that enables the creation of a fully captured computation graph suitable for Ahead-of-Time (AOT) deployment. This approach allows models to be saved as reusable, serialized graphs that can be used in different runtime environments without needing the original Python code.\n",
        "\n",
        "`torch.export()` differs from `torch.compile()` by requiring a fully traceable graph. While `torch.compile()` can fall back to Python runtime when encountering an untraceable operation, `torch.export()` enforces strict tracing requirements and raises errors if an operation cannot be traced.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "BOc0LJAnGMF3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn"
      ],
      "metadata": {
        "id": "79kxUsWuIHTW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "\n",
        "# **2. Limitations of `torch.export()`**\n",
        "\n",
        "## Graph Breaks\n",
        "\n",
        "`torch.export()` requires a fully traceable graph, and certain operations in Python or PyTorch are difficult to trace (e.g., dynamic control flow or custom functions). When `torch.export()` encounters an untraceable operation, it raises an error. This contrasts with `torch.compile()`, which allows \"graph breaks\" and continues to run the operation in Python.\n",
        "\n",
        "The example below demonstrates a graph break using a custom function that `torch.export()` cannot trace.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "3LJtNgtmH8iM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Wrap the untraceable function in a Module subclass\n",
        "class UntraceableModule(nn.Module):\n",
        "    def forward(self, x):\n",
        "        if x.item() > 0:\n",
        "            return x * 2\n",
        "        else:\n",
        "            return x\n",
        "\n",
        "# Define the model with the wrapped untraceable function\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(10, 10),\n",
        "    nn.ReLU(),\n",
        "    UntraceableModule()  # Now it’s a Module subclass\n",
        ")\n",
        "\n",
        "# Attempt to export the model with args as a tuple\n",
        "try:\n",
        "    exported_model = torch.export.export(model, (torch.randn(10, 10),))  # Wrap input in a tuple\n",
        "except Exception as e:\n",
        "    print(f\"Error during export: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efU-f9yeHet9",
        "outputId": "6137bfb3-44db-42b9-dc76-68cc4f1fcbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "E1108 19:01:57.258000 452 torch/export/_trace.py:1003] always_classified is unsupported.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error during export: Failed running call_method item(*(FakeTensor(..., size=(10, 10), grad_fn=<ReluBackward0>),), **{}):\n",
            "a Tensor with 100 elements cannot be converted to Scalar\n",
            "\n",
            "from user code:\n",
            "   File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/external_utils.py\", line 40, in inner\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"<ipython-input-15-ad941b7299ad>\", line 7, in forward\n",
            "    if x.item() > 0:\n",
            "\n",
            "Set TORCH_LOGS=\"+dynamo\" and TORCHDYNAMO_VERBOSE=1 for more information\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Using `torch.cond` for Data-Dependent Control Flow\n",
        "\n",
        "In PyTorch, control flow operations that depend on tensor data or shapes are challenging for a tracing compiler because it would require generating code paths for all possible conditions. `torch.export()` supports data-dependent control flow by using `torch.cond`, a specialized operator for handling `if-else` logic in a traceable way.\n",
        "\n",
        "The example below illustrates how `torch.cond` can be used to handle conditional operations based on tensor shapes.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "2VaSuDHTIP8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        if x.shape[0] > 5:\n",
        "            return x + 1\n",
        "        else:\n",
        "            return x - 1\n",
        "\n",
        "example_input = (torch.rand(10, 2),)\n",
        "exported_model = torch.export.export(ConditionalModel(), example_input)\n",
        "print(exported_model)  # The condition does not appear due to shape specialization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyrB9-dWHxrx",
        "outputId": "a4cf8a34-f862-47ca-ccd0-4762e7bcbbdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExportedProgram:\n",
            "    class GraphModule(torch.nn.Module):\n",
            "        def forward(self, x: \"f32[10, 2]\"):\n",
            "             # File: <ipython-input-16-34662d95aa80>:4 in forward, code: return x + 1\n",
            "            add: \"f32[10, 2]\" = torch.ops.aten.add.Tensor(x, 1);  x = None\n",
            "            return (add,)\n",
            "            \n",
            "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None)])\n",
            "Range constraints: {}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **3. Comparison with `torch.compile()`**\n",
        "\n",
        "Both `torch.export()` and `torch.compile()` rely on PyTorch's underlying graph-capturing technology (specifically TorchDynamo and FX), but they serve different purposes and offer different levels of flexibility:\n",
        "\n",
        "- **Purpose**:\n",
        "  - `torch.compile()` is primarily designed for Just-In-Time (JIT) compilation, making it suitable for experimentation, training, and general-purpose optimizations.\n",
        "  - `torch.export()`, on the other hand, is meant for Ahead-of-Time (AOT) deployment, producing a fully serialized graph that can be reused across environments and devices without needing Python runtime.\n",
        "\n",
        "- **Flexibility with Untraceable Parts**:\n",
        "  - `torch.compile()` is more flexible; if it encounters operations that cannot be traced (i.e., *graph breaks*), it will allow those parts to run in Python using the default runtime, effectively skipping full tracing for those segments.\n",
        "  - `torch.export()` requires a fully captured, end-to-end graph without untraceable segments, so it will throw an error if it encounters any untraceable parts. This ensures that the exported graph is self-contained and not dependent on Python runtime.\n",
        "\n",
        "- **Use Case Differences**:\n",
        "  - **torch.compile()** is suitable for training and inferencing with high flexibility, as it can handle models with complex, dynamic control flow.\n",
        "  - **torch.export()** is more suited for deploying models in production where a standalone, traceable graph is required, and it’s acceptable to rewrite code for strict traceability.\n",
        "\n",
        "The following example demonstrates how `torch.compile()` can handle certain graph breaks that would otherwise cause issues with `torch.export()`.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "jhnXl3UDHUze"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple model with dynamic control flow\n",
        "class DynamicControlModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        if x.shape[0] > 5:  # Condition on shape, which could vary in different inputs\n",
        "            return x + 1\n",
        "        else:\n",
        "            return x - 1\n",
        "\n",
        "# Compile the model with torch.compile()\n",
        "compiled_model = torch.compile(DynamicControlModel())\n",
        "\n",
        "# Run the model to observe flexibility in handling control flow\n",
        "output = compiled_model(torch.randn(10, 10))  # torch.compile() can handle this flexible runtime\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hqxw_KPIHQ-X",
        "outputId": "0e97e2a0-721c-4102-d447-c72ac3a900a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.2291, -0.0506,  0.8906,  1.7823,  1.0859,  1.3073,  1.0086,  1.6430,\n",
            "          1.7016,  1.5336],\n",
            "        [ 1.1889,  0.0981,  1.3893,  2.7723,  2.0871,  0.7327,  2.1079,  0.9839,\n",
            "          0.5456,  1.2943],\n",
            "        [-0.4916, -0.2345,  1.2255,  2.0321,  1.6659,  2.1579,  1.8077,  0.8497,\n",
            "          0.4578,  0.0847],\n",
            "        [ 3.1650,  0.7750,  0.8805,  1.0825, -0.0762,  1.2252,  0.4562,  0.7978,\n",
            "          2.6023,  3.1258],\n",
            "        [ 0.6714, -0.5205,  2.0268,  0.6293,  0.5531,  0.3261,  1.3184,  0.7365,\n",
            "          1.5657,  2.8883],\n",
            "        [ 0.5795,  2.3762,  1.1510,  0.2447, -0.1941,  3.5267,  0.9365,  2.6206,\n",
            "          0.7494, -1.7936],\n",
            "        [-1.4705, -1.1605,  1.6978,  1.1631,  0.7249, -0.0164,  1.0046,  0.9659,\n",
            "          1.6990,  0.0542],\n",
            "        [ 0.7296,  1.1931,  0.1662,  1.2432,  1.6827, -0.3016,  1.1667,  2.8555,\n",
            "          2.3856,  1.0344],\n",
            "        [ 1.2813,  1.3869,  0.2535,  2.5452,  2.7733,  0.9811,  3.2994,  0.0365,\n",
            "          0.4662,  0.6400],\n",
            "        [-0.3389,  1.5424,  0.7425,  1.1430, -1.6946,  0.8472,  1.8330, -0.2962,\n",
            "          0.3976,  0.2992]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **4. Using Non-Strict Mode for Workarounds**\n",
        "\n",
        "`torch.export()` offers a \"non-strict\" mode for cases where certain operations cannot be traced, but don’t affect the core computations. Non-strict mode (`strict=False`) allows `torch.export()` to bypass these operations by using ProxyTensors, capturing only the essential operations for the model’s computation.\n",
        "\n",
        "In this example, we create a custom context manager, `ContextManager`, which `torch.export()` cannot trace in strict mode. However, setting `strict=False` enables `torch.export()` to bypass this untraceable part, allowing the export to succeed.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MEKzGkwpHJpk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "# Define a model with an in-place operation\n",
        "class InPlaceOperationModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        x += 1  # In-place operation, usually untraceable in strict mode\n",
        "        return x\n",
        "\n",
        "# Attempt to export the model in strict mode (this should fail)\n",
        "print(\"Attempting export with strict=True\")\n",
        "try:\n",
        "    exported_model_strict = torch.export.export(InPlaceOperationModel(), (torch.ones(3, 3),), strict=True)\n",
        "    print(\"Export successful with strict=True (unexpected)\")\n",
        "except Exception as e:\n",
        "    print(f\"Error with strict=True: {e}\")\n",
        "\n",
        "# Attempt to export the model in non-strict mode (this should succeed)\n",
        "print(\"Attempting export with strict=False\")\n",
        "try:\n",
        "    exported_model_non_strict = torch.export.export(InPlaceOperationModel(), (torch.ones(3, 3),), strict=False)\n",
        "    print(\"Export successful with strict=False\")\n",
        "except Exception as e:\n",
        "    print(f\"Error with strict=False: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6GqLiecQHFrZ",
        "outputId": "dfae8760-1b14-4cf2-ce4a-3dd433706032"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting export with strict=True\n",
            "Export successful with strict=True (unexpected)\n",
            "Attempting export with strict=False\n",
            "Export successful with strict=False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **5. Expressing Dynamic Shapes with `torch.export.Dim()`**\n",
        "\n",
        "By default, `torch.export()` specializes on input tensor shapes, assuming they remain static. However, dimensions like batch size may vary between runs. The `torch.export.Dim()` API allows these dimensions to be marked as dynamic, enabling the exported model to adapt to varying input shapes.\n",
        "\n",
        "In the example below, we specify that the first dimension (typically the batch size) is dynamic.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "WX8A9HopHAKQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.export import Dim, export\n",
        "\n",
        "class DynamicShapeModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x + torch.ones_like(x)\n",
        "\n",
        "batch_dim = Dim(\"batch\")\n",
        "dynamic_shapes = {\"x\": {0: batch_dim}}\n",
        "exported_model = export(DynamicShapeModel(), (torch.randn(32, 64),), dynamic_shapes=dynamic_shapes)\n",
        "print(exported_model)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqaX911aG5z1",
        "outputId": "773aa855-9d12-40c2-ae09-4613687f3ec5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExportedProgram:\n",
            "    class GraphModule(torch.nn.Module):\n",
            "        def forward(self, x: \"f32[s0, 64]\"):\n",
            "             # File: <ipython-input-19-a84b8747708b>:5 in forward, code: return x + torch.ones_like(x)\n",
            "            ones_like: \"f32[s0, 64]\" = torch.ops.aten.ones_like.default(x, pin_memory = False)\n",
            "            add_3: \"f32[s0, 64]\" = torch.ops.aten.add.Tensor(x, ones_like);  x = ones_like = None\n",
            "            return (add_3,)\n",
            "            \n",
            "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add_3'), target=None)])\n",
            "Range constraints: {s0: VR[0, int_oo]}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **6. Serialization with `torch.export.save()` and `torch.export.load()`**\n",
        "\n",
        "`torch.export()` includes built-in support for serialization, allowing models to be saved as `.pt2` files and reloaded later. This makes it easy to share or deploy models without needing access to the original Python code.\n",
        "\n",
        "The following example demonstrates how to save and load an exported program.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "JwcsEFLfG1l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x + 10\n",
        "\n",
        "exported_model = export(SimpleModel(), (torch.randn(5),))\n",
        "torch.export.save(exported_model, 'model.pt2')\n",
        "loaded_model = torch.export.load('model.pt2')\n",
        "print(loaded_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Syx_tJKkGnQm",
        "outputId": "5a476078-a05c-4403-ece1-30a7dd8e976e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ExportedProgram:\n",
            "    class GraphModule(torch.nn.Module):\n",
            "        def forward(self, x: \"f32[5]\"):\n",
            "             # File: <ipython-input-20-b6dc52742b59>:3 in forward, code: return x + 10\n",
            "            add: \"f32[5]\" = torch.ops.aten.add.Tensor(x, 10);  x = None\n",
            "            return (add,)\n",
            "            \n",
            "Graph signature: ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.USER_INPUT: 1>, arg=TensorArgument(name='x'), target=None, persistent=None)], output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>, arg=TensorArgument(name='add'), target=None)])\n",
            "Range constraints: {}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# **7. Summary and Key Takeaways**\n",
        "\n",
        "- `torch.export()` is designed for Ahead-of-Time (AOT) deployment and requires fully traceable graphs.\n",
        "- Compared to `torch.compile()`, `torch.export()` is stricter, enforcing full graph tracing, making it suitable for production and deployment.\n",
        "- Dynamic dimensions, such as batch size, can be specified using `torch.export.Dim()`, making the model adaptable to inputs with varying shapes.\n",
        "- Non-strict mode provides flexibility to bypass unsupported features, allowing for more tracing options without modifying core tensor computations.\n",
        "- `.pt2` serialization enables easy sharing and deployment of models without needing the original code, making `torch.export()` powerful for PyTorch model deployment.\n",
        "\n",
        "This notebook covers the usage, limitations, and advantages of `torch.export()`, offering a foundation for efficient model deployment with PyTorch.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2amwsNR0Gs92"
      }
    }
  ]
}