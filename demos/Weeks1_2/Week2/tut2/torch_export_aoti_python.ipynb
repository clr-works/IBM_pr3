{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# For tips on running notebooks in Google Colab, see\n",
    "# https://pytorch.org/tutorials/beginner/colab\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.meta description=\"An end-to-end example of how to use AOTInductor for Python runtime.\" keywords=\"torch.export, AOTInductor, torch._inductor.aot_compile, torch._export.aot_load\"}\n",
    ":::\n",
    "\n",
    "`torch.export` AOTInductor Tutorial for Python runtime (Beta)\n",
    "=============================================================\n",
    "\n",
    "**Author:** Ankith Gunapal, Bin Bao, Angela Yi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #e94f3b; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>WARNING:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p><code>torch._inductor.aot_compile</code> and <code>torch._export.aot_load</code> are in Beta status and are subject to backwards compatibilitybreaking changes. This tutorial provides an example of how to use these APIs for model deployment using Python runtime.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "It has been shown\n",
    "[previously](https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html#)\n",
    "how AOTInductor can be used to do Ahead-of-Time compilation of PyTorch\n",
    "exported models by creating a shared library that can be run in a\n",
    "non-Python environment.\n",
    "\n",
    "In this tutorial, you will learn an end-to-end example of how to use\n",
    "AOTInductor for Python runtime. We will look at how to use\n",
    "`torch._inductor.aot_compile`{.interpreted-text role=\"func\"} along with\n",
    "`torch.export.export`{.interpreted-text role=\"func\"} to generate a\n",
    "shared library. Additionally, we will examine how to execute the shared\n",
    "library in Python runtime using\n",
    "`torch._export.aot_load`{.interpreted-text role=\"func\"}. You will learn\n",
    "about the speed up seen in the first inference time using AOTInductor,\n",
    "especially when using `max-autotune` mode which can take some time to\n",
    "execute.\n",
    "\n",
    "**Contents**\n",
    "\n",
    "::: {.contents local=\"\"}\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites\n",
    "=============\n",
    "\n",
    "-   PyTorch 2.4 or later\n",
    "-   Basic understanding of `torch.export` and AOTInductor\n",
    "-   Complete the [AOTInductor: Ahead-Of-Time Compilation for\n",
    "    Torch.Export-ed\n",
    "    Models](https://pytorch.org/docs/stable/torch.compiler_aot_inductor.html#)\n",
    "    tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you will learn\n",
    "===================\n",
    "\n",
    "-   How to use AOTInductor for python runtime.\n",
    "-   How to use `torch._inductor.aot_compile`{.interpreted-text\n",
    "    role=\"func\"} along with `torch.export.export`{.interpreted-text\n",
    "    role=\"func\"} to generate a shared library\n",
    "-   How to run a shared library in Python runtime using\n",
    "    `torch._export.aot_load`{.interpreted-text role=\"func\"}.\n",
    "-   When do you use AOTInductor for python runtime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Compilation\n",
    "=================\n",
    "\n",
    "We will use the TorchVision pretrained [ResNet18]{.title-ref} model and\n",
    "TorchInductor on the exported PyTorch program using\n",
    "`torch._inductor.aot_compile`{.interpreted-text role=\"func\"}.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>This API also supports <code>torch.compile</code> options like <code>mode</code>This means that if used on a CUDA enabled device, you can, for example, set <code>&quot;max_autotune&quot;: True</code>which leverages Triton based matrix multiplications &amp; convolutions, and enables CUDA graphs by default.</p>\n",
    "\n",
    "</div>\n",
    "\n",
    "We also specify `dynamic_shapes` for the batch dimension. In this\n",
    "example, `min=2` is not a bug and is explained in [The 0/1\n",
    "Specialization\n",
    "Problem](https://docs.google.com/document/d/16VPOa3d-Liikf48teAOmxLc92rgvJdfosIy-yoT38Io/edit?fbclid=IwAR3HNwmmexcitV0pbZm_x1a4ykdXZ9th_eJWK-3hBtVgKnrkmemz6Pm5jRQ#heading=h.ez923tomjvyk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision.models import ResNet18_Weights, resnet18\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT)\n",
    "model.eval()\n",
    "\n",
    "with torch.inference_mode():\n",
    "\n",
    "    # Specify the generated shared library path\n",
    "    aot_compile_options = {\n",
    "            \"aot_inductor.output_path\": os.path.join(os.getcwd(), \"resnet18_pt2.so\"),\n",
    "    }\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        aot_compile_options.update({\"max_autotune\": True})\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "\n",
    "    model = model.to(device=device)\n",
    "    example_inputs = (torch.randn(2, 3, 224, 224, device=device),)\n",
    "\n",
    "    # min=2 is not a bug and is explained in the 0/1 Specialization Problem\n",
    "    batch_dim = torch.export.Dim(\"batch\", min=2, max=32)\n",
    "    exported_program = torch.export.export(\n",
    "        model,\n",
    "        example_inputs,\n",
    "        # Specify the first dimension of the input x as dynamic\n",
    "        dynamic_shapes={\"x\": {0: batch_dim}},\n",
    "    )\n",
    "    so_path = torch._inductor.aot_compile(\n",
    "        exported_program.module(),\n",
    "        example_inputs,\n",
    "        # Specify the generated shared library path\n",
    "        options=aot_compile_options\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model Inference in Python\n",
    "=========================\n",
    "\n",
    "Typically, the shared object generated above is used in a non-Python\n",
    "environment. In PyTorch 2.3, we added a new API called\n",
    "`torch._export.aot_load`{.interpreted-text role=\"func\"} to load the\n",
    "shared library in the Python runtime. The API follows a structure\n",
    "similar to the `torch.jit.load`{.interpreted-text role=\"func\"} API . You\n",
    "need to specify the path of the shared library and the device where it\n",
    "should be loaded.\n",
    "\n",
    "<div style=\"background-color: #54c7ec; color: #fff; font-weight: 700; padding-left: 10px; padding-top: 5px; padding-bottom: 5px\"><strong>NOTE:</strong></div>\n",
    "\n",
    "<div style=\"background-color: #f3f4f7; padding-left: 10px; padding-top: 10px; padding-bottom: 10px; padding-right: 10px\">\n",
    "\n",
    "<p>In the example above, we specified <code>batch_size=1</code> for inference and it still functions correctly even though we specified <code>min=2</code> in<code>torch.export.export</code>.</p>\n",
    "\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_so_path = os.path.join(os.getcwd(), \"resnet18_pt2.so\")\n",
    "\n",
    "model = torch._export.aot_load(model_so_path, device)\n",
    "example_inputs = (torch.randn(1, 3, 224, 224, device=device),)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    output = model(example_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to use AOTInductor for Python Runtime\n",
    "==========================================\n",
    "\n",
    "One of the requirements for using AOTInductor is that the model\n",
    "shouldn\\'t have any graph breaks. Once this requirement is met, the\n",
    "primary use case for using AOTInductor Python Runtime is for model\n",
    "deployment using Python. There are mainly two reasons why you would use\n",
    "AOTInductor Python Runtime:\n",
    "\n",
    "-   `torch._inductor.aot_compile` generates a shared library. This is\n",
    "    useful for model versioning for deployments and tracking model\n",
    "    performance over time.\n",
    "-   With `torch.compile`{.interpreted-text role=\"func\"} being a JIT\n",
    "    compiler, there is a warmup cost associated with the first\n",
    "    compilation. Your deployment needs to account for the compilation\n",
    "    time taken for the first inference. With AOTInductor, the\n",
    "    compilation is done offline using `torch.export.export` &\n",
    "    `torch._indutor.aot_compile`. The deployment would only load the\n",
    "    shared library using `torch._export.aot_load` and run inference.\n",
    "\n",
    "The section below shows the speedup achieved with AOTInductor for first\n",
    "inference\n",
    "\n",
    "We define a utility function `timed` to measure the time taken for\n",
    "inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "def timed(fn):\n",
    "    # Returns the result of running `fn()` and the time it took for `fn()` to run,\n",
    "    # in seconds. We use CUDA events and synchronization for accurate\n",
    "    # measurement on CUDA enabled devices.\n",
    "    if torch.cuda.is_available():\n",
    "        start = torch.cuda.Event(enable_timing=True)\n",
    "        end = torch.cuda.Event(enable_timing=True)\n",
    "        start.record()\n",
    "    else:\n",
    "        start = time.time()\n",
    "\n",
    "    result = fn()\n",
    "    if torch.cuda.is_available():\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "    else:\n",
    "        end = time.time()\n",
    "\n",
    "    # Measure time taken to execute the function in miliseconds\n",
    "    if torch.cuda.is_available():\n",
    "        duration = start.elapsed_time(end)\n",
    "    else:\n",
    "        duration = (end - start) * 1000\n",
    "\n",
    "    return result, duration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets measure the time for first inference using AOTInductor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch._dynamo.reset()\n",
    "\n",
    "model = torch._export.aot_load(model_so_path, device)\n",
    "example_inputs = (torch.randn(1, 3, 224, 224, device=device),)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _, time_taken = timed(lambda: model(example_inputs))\n",
    "    print(f\"Time taken for first inference for AOTInductor is {time_taken:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets measure the time for first inference using `torch.compile`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "torch._dynamo.reset()\n",
    "\n",
    "model = resnet18(weights=ResNet18_Weights.DEFAULT).to(device)\n",
    "model.eval()\n",
    "\n",
    "model = torch.compile(model)\n",
    "example_inputs = torch.randn(1, 3, 224, 224, device=device)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    _, time_taken = timed(lambda: model(example_inputs))\n",
    "    print(f\"Time taken for first inference for torch.compile is {time_taken:.2f} ms\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there is a drastic speedup in first inference time using\n",
    "AOTInductor compared to `torch.compile`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "==========\n",
    "\n",
    "In this recipe, we have learned how to effectively use the AOTInductor\n",
    "for Python runtime by compiling and loading a pretrained `ResNet18`\n",
    "model using the `torch._inductor.aot_compile` and\n",
    "`torch._export.aot_load` APIs. This process demonstrates the practical\n",
    "application of generating a shared library and running it within a\n",
    "Python environment, even with dynamic shape considerations and\n",
    "device-specific optimizations. We also looked at the advantage of using\n",
    "AOTInductor in model deployments, with regards to speed up in first\n",
    "inference time.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aoti_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
